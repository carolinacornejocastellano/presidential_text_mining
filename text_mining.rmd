---
title: "_Palabras que se las lleva el viento_"
subtitle: "Exploring presidential inauguration speeches of the last decade"
output: html_document
---

## Introduction

This is a project to explore the priorities of Peruvian governments in the last decade. The data used is the presidential inauguration speeches of the last decade. The speeches are available in the website of the Peruvian Congress. The speeches are in Spanish, so I used Google Translate to translate them to English. The speeches are available in the folder "data".

## Data

```{r load libraries}
libraries <- c("tidyverse", 
               "tidytext", 
               "wordcloud", 
               "RColorBrewer", 
               "reshape2", 
               "igraph", 
               "ggraph", 
               "widyr", 
               "tm", 
               "quanteda", 
               "quanteda.textplots", 
               "topicmodels")

for (lib in libraries) {
  suppressPackageStartupMessages(library(lib,
    character.only = TRUE
  ))
}
rm(lib, libraries)
```

```{r download speeches}
presidents <-
  c("Dina Boluarte", 
    "Pedro Castillo", 
    "Francisco Sagasti", 
    "Martin Vizcarra", 
    "Pedro Pablo Kuczynski", 
    "Ollanta Humala", 
    "Alejandro Toledo", 
    "Valentin Paniagua", 
    "Alberto Fujimori second term", 
    "Alberto Fujimori first term")

links <- paste0(
  "https://raw.githubusercontent.com/cornellano/presidential_text_mining/main/",
  tolower(gsub(" ", "_", presidents)), ".md")
```

```{r}
df <- tibble(
  president = presidents,
  date = c("2022-12-07", 
          "2021-07-28", 
          "2020-11-17", 
          "2018-03-23", 
          "2016-07-28",
          "2011-07-28", 
          "2001-07-28", 
          "2000-11-11", 
          "1995-07-28",
          "1990-07-28"), 
  url = links
)
```

```{r}
# Read speeches
df$speech <- df$url %>% 
    map_chr(~ read_lines(.x, 
      locale = locale(encoding = "UTF-8")) %>% 
        paste(collapse = "\n"))

# delete URLs from df
df$url <- NULL
```

## Data preparation
Now we will tokenize the speeches and remove stopwords:

```{r store stopwords in Spanish as a vector}
stopwords_es <- stopwords::stopwords("es",
  source = "stopwords-iso"
)
```

```{r}
df <- df %>% 
  unnest_tokens(word, speech) %>% 
  filter(!word %in% stopwords_es)
```

```{r general word ranking}
df %>%
  count(word,
    sort = TRUE
  )
```

```{r}
# Show the most common words in the speeches of each president
df %>% 
  count(president, 
    word, 
    sort = TRUE
  ) %>% 
  filter(n > 5) %>% 
  ggplot(aes(x = reorder(word, n), 
            y = n, 
            fill = president)) +
  geom_col() +
    facet_wrap(~president, scales = "free") +
    coord_flip() +
    theme_minimal() +
    labs(
      x = "Word",
      y = "Frequency", 
      title = "Most Common Words in Presidential Inauguration Speeches", 
      subtitle = "Last Decade", 
      fill = "President"
    )
```

```{r}


```


## N-grams
We will analyze the most common bigrams:

```{r bigrams}
bigrams <- df %>%
  unnest_tokens(bigram,
    word,
    token = "ngrams",
     n = 2
  )
```

```{r}
bigram_sep <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigram_counts <- bigram_sep %>%
  count(president,
    word1, 
    word2,
    sort = TRUE
  )
```

```{r Plot the most common bigrams}
bigram_counts %>%
  filter(n > 5) %>%
  ggplot(aes(x = reorder(word1, n), 
            y = n, 
            fill = president)) +
  geom_col() +
    facet_wrap(~president, scales = "free") +
    coord_flip() +
    theme_minimal() +
    labs(
      x = "Bigram",
      y = "Frequency", 
      title = "Most Common Bigrams in Presidential Inauguration Speeches", 
      subtitle = "Last Decade", 
      fill = "President"
    )

```





## Sentiment analysis

## Latent Dirichlet Allocation

```{r}
