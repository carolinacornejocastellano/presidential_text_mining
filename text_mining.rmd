---
title: "_Palabras que se las lleva el viento_"
subtitle: "Exploring presidential inauguration speeches of the last decade"
output: html_document
---

## Introduction

This is a project to explore the priorities of Peruvian governments in the last decade. The data used is the presidential inauguration speeches of the last decade. The speeches are available in the website of the Peruvian Congress. The speeches are in Spanish, so I used Google Translate to translate them to English. The speeches are available in the folder "data".

## Data

```{r load libraries}
libraries <- c(
  "tidyverse",
  "tidytext",
  "textdata",
  "wordcloud",
  "RColorBrewer",
  "reshape2",
  "igraph",
  "ggraph",
  "widyr",
  "tm",
  "quanteda",
  "quanteda.textplots",
  "topicmodels",
  "syuzhet",
  "SentimentAnalysis"
)

for (lib in libraries) {
  suppressPackageStartupMessages(library(lib,
    character.only = TRUE
  ))
}
rm(lib, libraries)
```

```{r download speeches}
presidents <-
  c("Dina Boluarte", 
    "Pedro Castillo", 
    "Francisco Sagasti", 
    "Martin Vizcarra", 
    "Pedro Pablo Kuczynski", 
    "Ollanta Humala", 
    "Alejandro Toledo", 
    "Valentin Paniagua", 
    "Alberto Fujimori second term", 
    "Alberto Fujimori first term")

links <- paste0(
  "https://raw.githubusercontent.com/cornellano/presidential_text_mining/main/",
  tolower(gsub(" ", "_", presidents)), ".md")
```

```{r}
df <- tibble(
  president = presidents,
  date = c("2022-12-07", 
          "2021-07-28", 
          "2020-11-17", 
          "2018-03-23", 
          "2016-07-28",
          "2011-07-28", 
          "2001-07-28", 
          "2000-11-11", 
          "1995-07-28",
          "1990-07-28"), 
  url = links
)
```

```{r}
# Read speeches
df$speech <- df$url %>% 
    map_chr(~ read_lines(.x, 
      locale = locale(encoding = "UTF-8")) %>% 
        paste(collapse = "\n"))

# delete URLs from df
df$url <- NULL
```

## Data preparation
Now we will tokenize the speeches and remove stopwords:

```{r store stopwords in Spanish as a vector}
stopwords_es <- stopwords::stopwords("es",
  source = "stopwords-iso"
)
```

```{r delete stopwords from the original df}
df_no_stopwords <- df %>%
  unnest_tokens(word, speech) %>%
  anti_join(data.frame(word = stopwords_es),
            by = "word") %>%
  group_by(president, date) %>%
  summarize(clean_speech = paste(word,
                                 collapse = " "))
```

```{r}
df_tokenized <- df_no_stopwords %>% 
  unnest_tokens(word, clean_speech) %>% 
  filter(!word %in% stopwords_es)
```

```{r general word ranking}
df_tokenized %>%
  count(word,
    sort = TRUE
  )
```

```{r plot the most common words in presidential speeches}
df_tokenized %>% 
  count(president, 
    word, 
    sort = TRUE
  ) %>% 
  filter(n > 7) %>% # words that each president used more than 7 times
  ggplot(aes(x = reorder(word, n), 
            y = n, 
            fill = president)) +
  geom_col() +
    facet_wrap(~president, 
    scales = "free") +
    coord_flip() +
    scale_fill_manual(values = brewer.pal(8, "BrBG")) +
    theme_minimal() +
    ggtitle("Most Common Words in Presidential Inauguration Speeches", 
            subtitle = "From 1990 to 2022") +
    labs(
      x = "Word",
      y = "Frequency", 
      fill = "President"
    ) +
    theme(
      plot.title = element_text(
        hjust = 0.5,
        size = 20,
        face = "bold"
      ),
      plot.subtitle = element_text(
        hjust = 0.5,
        size = 14
      )
    )
```

## N-grams


We will analyze the most common bigrams:

```{r most common bigrams in all speeches}
df_no_stopwords %>%
  unnest_tokens(bigram,
                clean_speech,
                token = "ngrams",
                n = 2) %>%
  filter(!is.na(bigram)) %>%
  count(bigram, sort = TRUE) %>% 
  head(20)
```

```{r}
bigrams <-  df_no_stopwords %>%
  unnest_tokens(bigram, 
                clean_speech, 
                token = "ngrams", 
                n = 2)

# Bigrams by president
bigram_counts <- bigrams %>%
  count(president, 
        bigram, 
        sort = TRUE)

# Top bigrams for each president
top_bigrams <- bigram_counts %>%
  group_by(president) %>%
  top_n(2, n) %>%
  arrange(president)

top_bigrams
```
The following plot does not show well in the inline preview. Please, click in "Show in New window" button for it to be displayed well:

```{r bigrams plot}
ggplot(top_bigrams, 
       aes(x = reorder(bigram, n), 
           y = n, 
           fill = president)) +
  geom_col() +
  facet_wrap(~ president, 
             scales = "free") +
  coord_flip() +
  scale_fill_manual(values = brewer.pal(10, "BrBG")) +
  theme_minimal() +
  ggtitle("Most Common Bigrams in Presidential Speeches", 
          subtitle = "From 1990 to 2022") +
  labs(x = "Bigram", 
       y = "Frequency", 
       fill = "President") +
  theme(plot.title = element_text(hjust = 0.5, 
                                  size = 20, 
                                  face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, 
                                     size = 14))

```


Now, let's take a look to the most common trigrams:

```{r}
df_no_stopwords %>%
  unnest_tokens(trigram,
                clean_speech,
                token = "ngrams",
                n = 3) %>%
  filter(!is.na(trigram)) %>%
  count(trigram, sort = TRUE) %>% 
  head(20)
```

```{r}
trigrams <-  df_no_stopwords %>%
  unnest_tokens(trigram, 
                clean_speech, 
                token = "ngrams", 
                n = 3)

# trigrams by president
trigram_counts <- trigrams %>%
  count(president, 
        trigram, 
        sort = TRUE)

# Top bigrams for each president
top_trigrams <- trigram_counts %>%
  group_by(president) %>%
  top_n(2, n) %>%
  arrange(president)

top_trigrams
```

```{r}
ggplot(top_trigrams, 
       aes(x = reorder(trigram, n), 
           y = n, 
           fill = president)) +
  geom_col() +
  facet_wrap(~ president, 
             scales = "free") +
  coord_flip() +
  scale_fill_manual(values = brewer.pal(10, "BrBG")) +
  theme_minimal() +
  ggtitle("Most Common Trigrams in Presidential Speeches", 
          subtitle = "From 1990 to 2022") +
  labs(x = "Trigram", 
       y = "Frequency", 
       fill = "President") +
  theme(plot.title = element_text(hjust = 0.5, 
                                  size = 20, 
                                  face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, 
                                     size = 14))
```
Tres de los cuatro NO fueron elegidos democráticamente, sino que asumieron el cargo de emergencia, no tienen trigrams identificados. Por esa razón, aparecen muchos, imposible de leer.

## Sentiment analysis

```{r }
sentiments_spanish <- analyzeSentiment(df_tokenized$word,
  language = "spanish"
)
head(sentiments_spanish)
```

```{r}
df_sentiments <- data.frame(df_tokenized$word,
  sentiment = convertToDirection(sentiments_spanish$SentimentGI))
df_sentiments
```

```{r}
table(df_sentiments$sentiment)
```

```{r}
df_sentiments %>%
  group_by(sentiment) %>%
  summarise(number = n()) %>% 
  ggplot(aes(x = sentiment,
            y=number)) +
  geom_bar(aes(fill=sentiment),
              stat = "identity") +
  scale_fill_manual(values = brewer.pal(4, "BrBG")) +
  theme_minimal() +
  ggtitle("Most Common Sentiments in Presidential Speeches", 
          subtitle = "From 1990 to 2022") +
  labs(x = "Sentiment", 
       y = "Frequency", 
       fill = "President") +
  theme(plot.title = element_text(hjust = 0.5, 
                                  size = 20, 
                                  face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, 
                                     size = 14))
```

Most common sentiments per president

```{r}

```

```{r}
df_sentiments %>%
  group_by(Year, President, sentiment) %>%
  summarise(number = n()) %>% 
  ggplot(aes(x = sentiment,
            y=number)) +
  geom_bar(aes(fill=President),
              stat = "identity") +
  scale_fill_manual(values = brewer.pal(length(unique(df_sentiments$President)), "Set3")) +
  theme_minimal() +
  ggtitle("Most Common Sentiments in Presidential Speeches", 
          subtitle = "From 1990 to 2022") +
  labs(x = "Sentiment", 
       y = "Frequency", 
       fill = "President") +
  theme(plot.title = element_text(hjust = 0.5, 
                                  size = 20, 
                                  face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, 
                                     size = 14))
```

## Latent Dirichlet Allocation

```{#r}
# Create a Document-Term Matrix (DTM)
dtm <- df_tokenized %>%
  count(president, word) %>%
  cast_dtm(president, word, n)

# LDA model with 5 topics
lda_model <- LDA(dtm, k = 5)

# Get the probabilities of words per topic
term_topic_prob <- tidy(lda_model, matrix = "beta")

# Top 10 words per topic
top_terms <- term_topic_prob %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

# Assign topics to documents
document_topic_prob <- tidy(lda_model, matrix = "gamma")

# Merge topics with original data
document_topic_merged <- df_no_stopwords %>%
  left_join(document_topic_prob,
    by = c("president" = "document")
  )

# Find the most probable topic for each president
president_topics <- document_topic_merged %>%
  group_by(president) %>%
  top_n(1, gamma) %>%
  select(president, topic)

```
