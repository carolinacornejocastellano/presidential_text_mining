---
title: "_Palabras que se las lleva el viento_"
subtitle: "Exploring presidential inauguration speeches of the last decade"
output: html_document
---

## Introduction

This is a project to explore the priorities of Peruvian governments in the last decade. The data used is the presidential inauguration speeches of the last decade. The speeches are available in the website of the Peruvian Congress. The speeches are in Spanish, so I used Google Translate to translate them to English. The speeches are available in the folder "data".

## Data

```{r load libraries}
libraries <- c("tidyverse", 
               "tidytext", 
               "wordcloud", 
               "RColorBrewer", 
               "reshape2", 
               "igraph", 
               "ggraph", 
               "widyr", 
               "tm", 
               "quanteda", 
               "quanteda.textplots", 
               "topicmodels")

for (lib in libraries) {
  suppressPackageStartupMessages(library(lib,
    character.only = TRUE
  ))
}
rm(lib, libraries)
```

```{r download speeches}
presidents <-
  c("Dina Boluarte", 
    "Pedro Castillo", 
    "Francisco Sagasti", 
    "Martin Vizcarra", 
    "Pedro Pablo Kuczynski", 
    "Ollanta Humala", 
    "Alejandro Toledo", 
    "Valentin Paniagua", 
    "Alberto Fujimori second term", 
    "Alberto Fujimori first term")

links <- paste0(
  "https://raw.githubusercontent.com/cornellano/presidential_text_mining/main/",
  tolower(gsub(" ", "_", presidents)), ".md")
```

```{r}
df <- tibble(
  president = presidents,
  date = c("2022-12-07", 
          "2021-07-28", 
          "2020-11-17", 
          "2018-03-23", 
          "2016-07-28",
          "2011-07-28", 
          "2001-07-28", 
          "2000-11-11", 
          "1995-07-28",
          "1990-07-28"), 
  url = links
)
```

```{r}
# Read speeches
df$speech <- df$url %>% 
    map_chr(~ read_lines(.x, 
      locale = locale(encoding = "UTF-8")) %>% 
        paste(collapse = "\n"))

# delete URLs from df
df$url <- NULL
```

## Data preparation
Now we will tokenize the speeches and remove stopwords:

```{r store stopwords in Spanish as a vector}
stopwords_es <- stopwords::stopwords("es",
  source = "stopwords-iso"
)
```

```{r delete stopwords from the original df}
df_no_stopwords <- df %>%
  unnest_tokens(word, speech) %>%
  anti_join(data.frame(word = stopwords_es),
            by = "word") %>%
  group_by(president, date) %>%
  summarize(clean_speech = paste(word,
                                 collapse = " "))
```
```{r}
df_tokenized <- df %>% 
  unnest_tokens(word, speech) %>% 
  filter(!word %in% stopwords_es)
```

```{r general word ranking}
df_tokenized %>%
  count(word,
    sort = TRUE
  )
```

```{r plot the most common words in presidential speeches}
df_tokenized %>% 
  count(president, 
    word, 
    sort = TRUE
  ) %>% 
  filter(n > 7) %>% # words that each president used more than 7 times
  ggplot(aes(x = reorder(word, n), 
            y = n, 
            fill = president)) +
  geom_col() +
    facet_wrap(~president, 
    scales = "free") +
    coord_flip() +
    scale_fill_manual(values = brewer.pal(8, "BrBG")) +
    theme_minimal() +
    ggtitle("Most Common Words in Presidential Inauguration Speeches", 
            subtitle = "From 1990 to 2022") +
    labs(
      x = "Word",
      y = "Frequency", 
      fill = "President"
    ) +
    theme(
      plot.title = element_text(
        hjust = 0.5,
        size = 20,
        face = "bold"
      ),
      plot.subtitle = element_text(
        hjust = 0.5,
        size = 14
      )
    )
```

## N-grams


We will analyze the most common bigrams:

```{r most common bigrams in all speeches}
df_no_stopwords %>%
  unnest_tokens(bigram,
                clean_speech,
                token = "ngrams",
                n = 2) %>%
  filter(!is.na(bigram)) %>%
  count(bigram, sort = TRUE) %>% 
  head(20)
```

```{r}
bigrams <-  df_no_stopwords %>%
  unnest_tokens(bigram, 
                clean_speech, 
                token = "ngrams", 
                n = 2)

# Bigrams by president
bigram_counts <- bigrams %>%
  count(president, 
        bigram, 
        sort = TRUE)

# Top bigrams for each president
top_bigrams <- bigram_counts %>%
  group_by(president) %>%
  top_n(2, n) %>%
  arrange(president)

top_bigrams
```



## Sentiment analysis

## Latent Dirichlet Allocation
