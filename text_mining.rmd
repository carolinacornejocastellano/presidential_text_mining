---
title: "_Palabras que se las lleva el viento_"
subtitle: "Exploring presidential inauguration speeches of the last decade"
output: html_document
---

## Introduction

This is a project to explore the priorities of Peruvian governments in the last decade. The data used is the presidential inauguration speeches of the last decade. The speeches are available in the website of the Peruvian Congress. The speeches are in Spanish, so I used Google Translate to translate them to English. The speeches are available in the folder "data".

## Data

```{r load libraries}
libraries <- c("tidyverse", 
               "tidytext", 
               "wordcloud", 
               "RColorBrewer", 
               "reshape2", 
               "igraph", 
               "ggraph", 
               "widyr", 
               "tm", 
               "quanteda", 
               "quanteda.textplots", 
               "topicmodels")

for (lib in libraries) {
  suppressPackageStartupMessages(library(lib,
    character.only = TRUE
  ))
}
rm(lib, libraries)
```

```{r download speeches}
presidents <-
  c("Dina Boluarte", 
    "Pedro Castillo", 
    "Francisco Sagasti", 
    "Martin Vizcarra", 
    "Pedro Pablo Kuczynski", 
    "Ollanta Humala", 
    "Alejandro Toledo", 
    "Valentin Paniagua", 
    "Alberto Fujimori second term", 
    "Alberto Fujimori first term")

links <- paste0(
  "https://raw.githubusercontent.com/cornellano/presidential_text_mining/main/",
  tolower(gsub(" ", "_", presidents)), ".md")
```

```{r}
df <- tibble(
  president = presidents,
  date = c("2022-12-07", 
          "2021-07-28", 
          "2020-11-17", 
          "2018-03-23", 
          "2016-07-28",
          "2011-07-28", 
          "2001-07-28", 
          "2000-11-11", 
          "1995-07-28",
          "1990-07-28"), 
  url = links
)
```


```{r}
# Read speeches
df$speech <- df$url %>% 
    map_chr(~ read_lines(.x, 
      locale = locale(encoding = "UTF-8")) %>% 
    paste(collapse = "\n"))
```

## Data preparation
Now we will tokenize the speeches and remove stopwords:

```{r store stopwords in Spanish as a vector}
stopwords_es <- stopwords::stopwords("es",
  source = "stopwords-iso"
)
```

```{r}
df <- df %>% 
  unnest_tokens(word, speech) %>% 
  filter(!word %in% stopwords_es)

```

```{r general word ranking}
df %>%
  count(word,
    sort = TRUE
  )
```

```{r}
frequency <- 
  #regex to identify words and not _words_
  mutate(word = str_extract(word, "[a-z']+")) %>%
  #we count number of mentions of a word for an author
  count(author, word) %>%
  #we calculate proportion
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  #we reshape the dataframe
  #pivot wider means: more columns, less rows
  pivot_wider(names_from = author, values_from = proportion) %>%
  #pivot longer means: more rows, less columns
  pivot_longer(`BrontÃ« Sisters`:`H.G. Wells`,
               names_to = "author", values_to = "proportion")

frequency
```


## N-grams
We will analyze the most common bigrams:

```{r bigrams}
bigrams <- df %>%
  unnest_tokens(bigram,
    word,
    token = "ngrams",
     n = 2
  )
```

```{r}
bigram_sep <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigram_counts <- bigram_sep %>%
  count(president,
    word1, 
    word2,
    sort = TRUE
  )
```

```{r Plot the most common bigrams}
bigram_counts %>%
  filter(n > 5) %>%
  ggplot(aes(x = reorder(word1, n), 
            y = n, 
            fill = president)) +
  geom_col() +
    facet_wrap(~president, scales = "free") +
    coord_flip() +
    theme_minimal() +
    labs(
      x = "Bigram",
      y = "Frequency", 
      title = "Most Common Bigrams in Presidential Inauguration Speeches", 
      subtitle = "Last Decade", 
      fill = "President"
    )

```





## Sentiment analysis

## Latent Dirichlet Allocation

```{r}
